{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "429b7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import os, glob, re\n",
    "\n",
    "\n",
    "def df_tf(doc_lst = None):\n",
    "    \"\"\"gets doc freq and term frequency per document\n",
    "    params:\n",
    "        optional: a list of file names, if None uses glob to read all txt in directory \n",
    "    returns:\n",
    "        word_freq: frequency of each word\n",
    "        all_words: set of all words\"\"\"\n",
    "        \n",
    "    word_lst = [] # list of all words returned as set\n",
    "    doc_dict = {} # each doc paired with a list of ALL words in it as list\n",
    "    \n",
    "    if doc_lst is None:\n",
    "        path = os.getcwd() # get the directory we're in\n",
    "        doc_lst = glob.glob(os.path.join(path, '*.txt')) # change this if using different file extensions\n",
    "        \n",
    "    for f in doc_lst:\n",
    "        doc_name = os.path.split(f)[-1] # grab filename\n",
    "        doc_title = doc_name.split('.')[0]\n",
    "        with open(f, 'r') as read_file:\n",
    "            text = read_file.readlines() # read all lines, stored as list of lines\n",
    "            text = [x.lower().strip() for x in text] # lowercase and strip \\n, spacing\n",
    "            all_lines = \" \".join(text) # combine lines to one string\n",
    "            clean_text = re.sub(r'[^\\w\\s]','',all_lines) # pull out punctuation\n",
    "            word_lst.append(clean_text) # add new string to giant word list\n",
    "            doc_dict[doc_title] = sorted(clean_text.split()) # add sorted list of words per doc to doc_dict value\n",
    "\n",
    "    words = \" \".join(word_lst) # join each list of doc words to one giant string\n",
    "#     word_freq = {k : words.count(k) for k in set(words.split())} # slower than nltk\n",
    "    word_freq = FreqDist(words.split()) # get freq dist for entire vocab\n",
    "    all_words = sorted(set(x for x in words.split())) # split and sort vocab\n",
    "\n",
    "    return all_words, word_freq, doc_dict\n",
    "\n",
    "\n",
    "def make_term_doc_incidence(doc_dict, all_words):\n",
    "    \"\"\"\n",
    "    parameters: \n",
    "        doc_dict: dict with list of text per doc\n",
    "        all_words: set(vocab of all docs)\n",
    "    returns:\n",
    "        term document incidence matrix\n",
    "        one hot df if word in doc for all vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {\"word\" : all_words}\n",
    "    df = pd.DataFrame(d)\n",
    "    # append df with each doc and one hot encoding\n",
    "    for i, word in enumerate(all_words):\n",
    "        for doc in doc_dict.keys():\n",
    "            df.loc[i, doc] = np.where(word in doc_dict[doc], 1, 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_tf(doc_dict, all_words):\n",
    "    \"\"\"\n",
    "    parameters: \n",
    "        doc_dict: dict created earlier with list of text in each doc\n",
    "        all_words: set(vocab of all docs)\n",
    "    returns:\n",
    "        term frequency\n",
    "        count of word in each doc\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {\"word\" : all_words}\n",
    "    df = pd.DataFrame(d)\n",
    "    # append df with each doc and one hot encoding\n",
    "    for i, word in enumerate(all_words):\n",
    "        for doc in doc_dict.keys():\n",
    "            df.loc[i, doc] = np.where(word in doc_dict[doc], doc_dict[doc].count(word), 0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def weighted_tf(df_tf):\n",
    "    \"\"\"calculates weighted term freq per doc\n",
    "    params:\n",
    "        df_tf: raw term doc freq (actual word counts per doc)\n",
    "    returns:\n",
    "        new df with weighted tf\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = df_tf.columns[1:]\n",
    "    for doc in docs:\n",
    "        \n",
    "        df_tf[doc] = 1 + np.log10(df_tf[doc])\n",
    "        df_tf[doc] = np.where( - df_tf[doc] == np.inf, 0, df_tf[doc])\n",
    "\n",
    "    return df_tf\n",
    "                         \n",
    "\n",
    "def inv_df(df):\n",
    "    \"\"\"calculates inverse doc freq\n",
    "    params:\n",
    "         df: term document incidence\n",
    "    returns:\n",
    "        inverse document frequency\n",
    "        log10(number of docs/doc freq)\n",
    "    \"\"\"\n",
    "\n",
    "#     doc_freq = df.sum(axis = 1, numeric_only = True)\n",
    "    docs = df.columns[1:]\n",
    "    n = len(docs)\n",
    "    for doc in docs:\n",
    "        doc_freq = df[doc]\n",
    "        df[doc] = np.log10(n/doc_freq)\n",
    "        df[doc] = np.where(df[doc] == np. inf, 0, df[doc])\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def tf_idf(weighted_tf, idf):\n",
    "    \"\"\"calculates tf-idf \n",
    "    (1 + log10(tf)) * log10(n/df)\n",
    "    params:\n",
    "        df_tf = dataframe of doc freq and term freq\n",
    "        weighted_tf: weighed term freq\n",
    "        idf: inverse doc freq\n",
    "    returns:\n",
    "        tf-idf\n",
    "    \"\"\"\n",
    "    \n",
    "    df = weighted_tf\n",
    "    docs = df.columns[1:]\n",
    "    for doc in docs:\n",
    "        df[doc] = weighted_tf[doc] * idf[doc]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cos_sim(q, d):\n",
    "    \"\"\"calculates cosine similarity between vectors\"\"\"\n",
    "\n",
    "    euclid = lambda x: np.linalg.norm(x)\n",
    "    # skip calc if ==\n",
    "    if np.all(q == d):\n",
    "        return 1\n",
    "\n",
    "    dot_prod = np.dot(q,d)\n",
    "    denom = euclid(q) * euclid(d)\n",
    "\n",
    "    # avoid /0\n",
    "    if np.isclose(denom, 0, atol = 1e-32):\n",
    "        sim = 0\n",
    "    else:\n",
    "        sim = dot_prod / denom\n",
    "        \n",
    "    return sim\n",
    "\n",
    "def cos_sim_compare(tfidf):\n",
    "    \"\"\"\n",
    "    creates matrix of cosine similarities\n",
    "    params:\n",
    "        tfidf: term freq inverse doc freq\n",
    "    returns:\n",
    "        similarity matrix to compare docs using cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = tfidf.columns[1:]\n",
    "    sim_matrix = pd.DataFrame()\n",
    "        \n",
    "    for doc in docs:\n",
    "        for row in docs:\n",
    "            sim_matrix.loc[row, doc] = cos_sim(tfidf[row], tfidf[doc])\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def main(lst_of_docs = None):\n",
    "    \"\"\"runs nlp processing\n",
    "    params:\n",
    "        lst_of_docs: optional list of file names, if blank glob will choose all txt files in directory\n",
    "    returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    aw, fd, dd = df_tf(doc_lst = lst_of_docs)\n",
    "    df = make_term_doc_incidence(dd, aw)\n",
    "    tf = make_tf(dd, aw)\n",
    "    wtf = weighted_tf(tf)\n",
    "    idf = inv_df(df)\n",
    "    tfidf = tf_idf(wtf, idf)\n",
    "    cos_sim_matrix = cos_sim_compare(tfidf)\n",
    "    return cos_sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "eeb5c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mdt20\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\arraylike.py:364: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864301</td>\n",
       "      <td>0.310088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.864301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.260563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.310088</td>\n",
       "      <td>0.260563</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc1      doc2      doc3\n",
       "doc1  1.000000  0.864301  0.310088\n",
       "doc2  0.864301  1.000000  0.260563\n",
       "doc3  0.310088  0.260563  1.000000"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d9f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
